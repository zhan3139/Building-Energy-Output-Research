# -*- coding: utf-8 -*-
"""ANN vs SVM - Presentation Version

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oKGkj_Gvhf6tRj_wU1ti8mzMEZYjdiDQ

# Authors
* Andy Zhang (zhan3139@purdue.edu) - Purdue BS/MS in ME 2023 

# About
* compare and contrast SVM and ANN models with onsite HVAC data collected from EMC2 facility in Indianapolis 
* model outcome is a r2 value representing what percent of the variation of the output variable can be explained by the mode. r2 values range from 0 to 1 with greater values representing better results. 
* multiple different plots are also generated for each model to help visualize model accuracy 

# How to Use
* run each cell sequentially and follow prompts
* need to have a data file ready with relevant HVAC parameters
* example datafile will be sent to project sponsors, but at the time of writing i am unsure if I can attach a link to our data here

# Helpful Resources and Additional Reading
* correlation matrix -> https://datagy.io/python-correlation-matrix/
* SVM for classification and regression -> https://towardsdatascience.com/svm-support-vector-machine-for-classification-710a009f6873
* ANN for rergession -> https://thinkingneuron.com/using-artificial-neural-networks-for-regression-in-python/
* weights and activation functions explained -> https://towardsdatascience.com/weight-initialization-and-activation-functions-in-deep-learning-50aac05c3533

**load data**
"""

import pandas as pd
import io
import numpy as np
from google.colab import files

# prompt user input for name of dataset
datasetName = input("input the name of the csv dataset: ")
datasetNameCSV = datasetName + ".csv"
print(datasetNameCSV)

# load dataset from local device
uploaded = files.upload()
dataset = pd.read_csv(io.BytesIO(uploaded[datasetNameCSV]))

# dataset.head() prints out the first 5 rows of the data so we can double check everything looks good before proceeding 
dataset.head()

"""**add date, time, and weekday columns to dataset**

*   Run this step if 'DateAndTime' data isn't separated into 'year', 'month', 'day', 'hour'
*   This step can be **skipped** for **'Chiller and Weather_Nov.csv'** since Dikai handled this data preprocessing step inside the csv file
* This step should be **ran** for **Chiller and Weather - DateAndTime** since the DateAndTime exists in its native format. 
* Both datasets in the above two bullet points are provided as an example of each
* An additional dataset was used for testing called **Heating Model_inputs_dec**. This step can be skipped for it. 



"""

# change date and time column to type of to_datetime so we can use their functions 
dataset['DateAndTime'] = pd.to_datetime(dataset['DateAndTime'])

# break apart date, time, and day of week
dataset["Date"] = dataset["DateAndTime"].dt.date
dataset["Time"] = dataset["DateAndTime"].dt.time
dataset["DayOfWeek"] = dataset['DateAndTime'].dt.weekday

# add scaled value for time (int value between 0 and 1)

"""**Add DayOfWeek Status**
* regardless of dataset, **'DayOfWeek Status'** is a good input for the models. This is a T/F flag for Weekdays. Run this section and dataset.head() to double check that everything looks good.
"""

# add binary column 'DayOfWeek Status' with (1 == weekday) and (0 == weekend)
dataset['DayOfWeek Status'] = np.where(dataset['DayOfWeek'] <= 4, 1, 0)

# print out head to check
dataset.head()

"""**Generate Correlation Matrix** 
* correlation matrix is used to assist in selecting X (independent) and Y (dependent) parameters
* each pair of variables has a score assigned to it from -1 to 1. A Value of -1 indicates a strong, negative correlation. A value of 1 indicates a strong, positive correlation. For modeling, whether or not the correlation is positive or negative does not matter as long as it is strong. Thus, we are interested in the absolute value of the score the correlation matrix generates. 
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

## correlation matrix - each value in the matrix relates the row and colomn value, values closer to 1 or -1 represent strongest positive or negative correlation possible 

# make the correlation matrix plot larger for easier reading
plt.rcParams["figure.figsize"] = (14,12)

# build correlation matrix
matrix = dataset.corr().round(2)
sns.heatmap(matrix, annot=True, vmax=1, vmin=-1, center=0, cmap='vlag')
plt.show()

"""**specifications are made here based on correlation matrix**
- set *'corValue'* based on corrleation matrix. What appears to be a strong correlation value?
- set output variable in *'modelMatrix'*. This is what the model is trying to predict. For our project, this is typically Energy Consumption. 
- When the *modelMatrix* prints, double check that the second column does not have any **'NaN'** values. If it does, please try rerunning the cell. 
"""

# prompt user input
cv = input("input a number between 0 and 1.0 for corValue, the minimum correlation value: ")
outputVar = input("input the name of the output variable (case sensistive): ")

# filter strong correlations 
corValue = float(cv)   # set this value based off what correlation matrix plot, what value do you think represents a strong relationship between the output and input varaibles? 
matrix = matrix.unstack()
matrix = matrix[abs(matrix) >= corValue]                      

# specify output variable here
modelMatrix = matrix[outputVar]

# print out modelMatrix to double check values before moving on
print("\n")
print(modelMatrix)

"""**automatically set x and y parameters**
*   this step is based on what is set as the output variable and the minimum 'corValue'



"""

## y variables

# isolates output variable into yVal
yVal = modelMatrix.idxmax() # since output variable has the highest correlation with itself
print("Y parameter is: ", yVal, "\n")

# split dataset
y = pd.DataFrame(dataset[[yVal]])

## x variables

# removes the output variable from model matrix to process just x variables
xVal = modelMatrix.drop(yVal) 

# isolate x varaibles in xVal_list
xVal_iterator = xVal
xVal_list = []

for i in xVal_iterator:
  max = xVal.idxmax()
  xVal = xVal.drop(max)
  xVal_list.append(max)

print("X parameter(s): ", xVal_list,"\n")

"""**hardcoding any other input parameters of interest** 
- some input parameters will not be identified to have a strong correlations to the target parameters
- i.e the hour of the day could be of interest but have a weak correlation 
- *xVal_list.append('name of parameter')* --> add input parameters with this template, an example for *'DateAndTime'* is commented out below
"""

#xVal_list.append('DateAndTime')

print("Updated X parameter(s) : ",xVal_list)
print("Number of X parameter(s): ", len(xVal_list))

"""**data preprocessing**


*   these steps need to completed to format data appropriately for modeling


"""

# split dataset 
X = (dataset[xVal_list])

# standarization procedure for my ANN model #
from sklearn.preprocessing import StandardScaler
PredictorScaler=StandardScaler()
TargetVarScaler=StandardScaler()
# Storing the fit object for later reference
PredictorScalerFit=PredictorScaler.fit(X)
TargetVarScalerFit=TargetVarScaler.fit(y)
# Generating the standardized values of X and y
X=PredictorScalerFit.transform(X)
y=TargetVarScalerFit.transform(y)

# 80-90% training is standard, training = 1 - test_size = 1 - 0.2 = 0.8
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

"""**support vector machine (SVM)**
- Two metrics, MSQ and r2, are used to measure 'accuracy'. The important one is the r2 value since it allows us to compare SVM to ANN. 
- Mean Squared Error (MSQ) is average squared difference between the estimated values and true value
- r2 is known as the coefficient of determination and it measures how well the model predicts the outcome or dependent variable (the variable *outputVar*). r2 is a float that ranges from 0 to 1 representing a percent between 0 and 100%. 
"""

## import SVM libraries 
from sklearn import svm
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

"""***support vector machine (kernel=rbf)***"""

## support vector machine (kernel=rbf)

# build and train model 
model_svm = svm.SVR(cache_size=500,kernel='rbf')
model_svm.fit(X_train, y_train.ravel()) # two adjustments on y_train are done to remove the following error "A column-vector y was passed when a 1d array was expected"

# make prediction and analyze results
y_pred_svm = model_svm.predict(X_test)
mse_svm = mean_squared_error(y_test, y_pred_svm)
print("MSE for Support Vector Machine Model, kernel=rbf:", mse_svm)
print("the r2 score is: ",r2_score(y_test, y_pred_svm))

r2_rbf_percent = round(r2_score(y_test, y_pred_svm)*100, 2)
print(r2_rbf_percent,"% of the data can be explained by the SVM - kernel=rbf model")

# isolate (svm, kernel=rbf) data
y_pred_svm_rbf = y_pred_svm

## save variables for plotting - svm (kernel=rbf)

# Generating Predictions on testing data
Predictions = y_pred_svm_rbf
Predictions = Predictions.reshape(-1,1)

# Scaling the predicted (y model) data back to original price scale
Predictions_rbf = TargetVarScalerFit.inverse_transform(Predictions)

"""***support vector machine (kernel=linear)***"""

## support vector machine (kernel=linear)

# build and train model 
model_svm = svm.SVR(cache_size=500,kernel='linear')
model_svm.fit(X_train, y_train.ravel()) # two adjustments on y_train are done to remove the following error "A column-vector y was passed when a 1d array was expected"

# make prediction and analyze results
y_pred_svm = model_svm.predict(X_test)
mse_svm = mean_squared_error(y_test, y_pred_svm)
print("MSE for Support Vector Machine Model, kernel=linear:", mse_svm)
print("the r2 score is: ",r2_score(y_test, y_pred_svm))

r2_lin_percent = round(r2_score(y_test, y_pred_svm)*100, 2)
print(r2_lin_percent,"% of the data can be explained by the SVM - kernel=linear model")

# isolate (svm, kernel=linear) data
y_pred_svm_linear = y_pred_svm

## save variables for plotting - svm (kernel=linear)

# Generating Predictions on testing data
Predictions = y_pred_svm_linear
Predictions = Predictions.reshape(-1,1)

# Scaling the predicted (y model) data back to original price scale
Predictions_linear = TargetVarScalerFit.inverse_transform(Predictions)

"""***support vector machine (kernel=poly)***"""

## support vector machine (kernel=poly)

# build and train model 
model_svm = svm.SVR(cache_size=500,kernel='poly')
model_svm.fit(X_train, y_train.ravel()) # two adjustments on y_train are done to remove the following error "A column-vector y was passed when a 1d array was expected"

# make prediction and analyze results
y_pred_svm = model_svm.predict(X_test)
mse_svm = mean_squared_error(y_test, y_pred_svm)
print("MSE for Support Vector Machine Model, kernel=poly:", mse_svm)
print("the r2 score is: ",r2_score(y_test, y_pred_svm))

r2_poly_percent = round(r2_score(y_test, y_pred_svm)*100, 2)
print(r2_poly_percent,"% of the data can be explained by the SVM - kernel=poly model")

# isolate (svm, kernel=poly) data
y_pred_svm_poly = y_pred_svm

## save variables for plotting - svm (kernel=poly)

# Generating Predictions on testing data
Predictions = y_pred_svm_poly
Predictions = Predictions.reshape(-1,1)

# Scaling the predicted (y model) data back to original price scale
Predictions_poly = TargetVarScalerFit.inverse_transform(Predictions)

"""**Linear Regression Plot of Predicted and Actual Output Parameter Data for SVM**

"""

## need original output 

y_test_orig=TargetVarScalerFit.inverse_transform(y_test)

## Actual vs Predicted Regression plot (svm,kernel=rbf)
import seaborn as sns

x = y_test_orig
y = Predictions_rbf

# Plot the regression plot
sns.regplot(x, y, label='Predicted')

# Plot the line plot
#sns.lineplot(x, y, label='Predicted')
#sns.lineplot(x[1],y[1], label='Predicted')

# Add a title
plt.title('Actual vs. Predicted (svm,kernel=rbf)')

# Label the x-axis
plt.xlabel('actual')

# Label the y-axis
plt.ylabel('predicted')

# Add a legend
plt.legend()

# Show the plot
plt.show()

## Actual vs Predicted Regression plot (svm,kernel=linear)
import seaborn as sns

x = y_test_orig
y = Predictions_linear

# Plot the regression plot
sns.regplot(x, y, label='Predicted')

# Plot the line plot
#sns.lineplot(x, y, label='Predicted')
#sns.lineplot(x[1],y[1], label='Predicted')

# Add a title
plt.title('Actual vs. Predicted (svm,kernel=linear)')

# Label the x-axis
plt.xlabel('actual')

# Label the y-axis
plt.ylabel('predicted')

# Add a legend
plt.legend()

# Show the plot
plt.show()

## Actual vs Predicted Regression plot (svm,kernel=poly)
import seaborn as sns

x = y_test_orig
y = Predictions_poly

# Plot the regression plot
sns.regplot(x, y, label='Predicted')

# Plot the line plot
#sns.lineplot(x, y, label='Predicted')
#sns.lineplot(x[1],y[1], label='Predicted')

# Add a title
plt.title('Actual vs. Predicted (svm,kernel=predicted)')

# Label the x-axis
plt.xlabel('actual')

# Label the y-axis
plt.ylabel('predicted')

# Add a legend
plt.legend()

# Show the plot
plt.show()

"""**Figures of Expected and Model Results for SVM**
- each kernel is plotted separately
"""

## plot of expected vs model results 
import matplotlib.pyplot as plt

# prepare x axis and y original data for plotting against y predicted data 
x_axis = list(range(1, len(Predictions)+1))
y_test_orig = TargetVarScalerFit.inverse_transform(y_test)

# plot (svm,kernel=rbf)
plt.plot(x_axis, Predictions_rbf, 'o g')
plt.plot(x_axis, y_test_orig, 'o b')

title = '(svm,kernel=rbf) expected vs model results for ' + outputVar
plt.title(title)
plt.xlabel('datapoint')
plt.ylabel(outputVar)
plt.legend(['Model Predictions', 'Original Data'])

# plot (svm,kernel=linear)
plt.plot(x_axis, Predictions_linear, 'o g')
plt.plot(x_axis, y_test_orig, 'o b')

title = '(svm,kernel=linear) expected vs model results for ' + outputVar
plt.title(title)
plt.xlabel('datapoint')
plt.ylabel(outputVar)
plt.legend(['Model Predictions', 'Original Data'])

# plot (svm,kernel=poly)
plt.plot(x_axis, Predictions_poly, 'o g')
plt.plot(x_axis, y_test_orig, 'o b')

title = '(svm,kernel=poly) expected vs model results for ' + outputVar
plt.title(title)
plt.xlabel('datapoint')
plt.ylabel(outputVar)
plt.legend(['Model Predictions', 'Original Data'])

"""**Figures of Expected and Model Results for SVM - Every Nth Datapoint**
- each kernel is plotted separately
- only plots ever Nth Datapoint to make reading figures easier
"""

## plot of expected vs model results  - every nth value plotted
# sometimes plotting every datapoint makes it difficult to read the figure, use this section to generate an alternative figure with every nth datapoint

# user input for nth value
nth_stringInput = input('to plot every nth value, please provide an integer value for n (recommended: n = 100)')
nth = int(nth_stringInput)

# plot (svm,kernel=rbf)
x = x_axis[::nth]
y1 = Predictions_rbf[::nth]
y2 = y_test_orig[::nth]

plt.plot(x, y1, 'o g')
plt.plot(x, y2, 'o b')

title = '(svm,kernel=rbf) expected vs model results for ' + outputVar + " : every " + nth_stringInput + ' datapoints'
plt.title(title)
plt.xlabel('datapoint')
plt.ylabel(outputVar)
plt.legend(['Model Predictions', 'Original Data'])

# plot (svm,kernel=linear)
x = x_axis[::nth]
y1 = Predictions_linear[::nth]
y2 = y_test_orig[::nth]

plt.plot(x, y1, 'o g')
plt.plot(x, y2, 'o b')

title = '(svm,kernel=linear) expected vs model results for ' + outputVar + " : every " + nth_stringInput + ' datapoints'
plt.title(title)
plt.xlabel('datapoint')
plt.ylabel(outputVar)
plt.legend(['Model Predictions', 'Original Data'])

# plot (svm,kernel=poly)
x = x_axis[::nth]
y1 = Predictions_poly[::nth]
y2 = y_test_orig[::nth]

plt.plot(x, y1, 'o g')
plt.plot(x, y2, 'o b')

title = '(svm,kernel=poly) expected vs model results for ' + outputVar + " : every " + nth_stringInput + ' datapoints'
plt.title(title)
plt.xlabel('datapoint')
plt.ylabel(outputVar)
plt.legend(['Model Predictions', 'Original Data'])

"""**artificial neural network (ANN)**

- built with Keras, a neural network library for python    
"""

## Build Neural Network

Predictors = xVal_list

# Dependencies
import keras
from keras.models import Sequential
from keras.layers import Dense

# input_dim depends on len(xVal_list)
inputDemSize = len(xVal_list)

# scaling up or down the number of neurons in the hidden layers
hiddenNeurons = int(inputDemSize * 2.2)

# Neural network
model = Sequential()
model.add(Dense(hiddenNeurons, input_dim=inputDemSize, activation='relu'))
model.add(Dense(hiddenNeurons, activation='relu'))
model.add(Dense(hiddenNeurons, activation='relu'))
model.add(Dense(hiddenNeurons, activation='relu'))
model.add(Dense(hiddenNeurons, activation='relu'))
model.add(Dense(1, activation='linear'))

## Training Model 

# compile the model
model.compile(loss='mean_squared_error', optimizer='adam')

model.fit(X_train, y_train, epochs=10, batch_size=30)

## Generate Predictions

# Generating Predictions on testing data
Predictions=model.predict(X_test)
 
# Scaling the predicted (y model) data back to original price scale
Predictions=TargetVarScalerFit.inverse_transform(Predictions)

# Scaling the y_test data back to original price scale
y_test_orig=TargetVarScalerFit.inverse_transform(y_test)
 
# Scaling the x_test data back to original scale
Test_Data=PredictorScalerFit.inverse_transform(X_test)
 
TestingData=pd.DataFrame(data=Test_Data, columns=Predictors)
TestingData['Energy Output']=y_test_orig
TestingData['Predicted Energy Output']=Predictions
TestingData.head()

"""**Accuracy of ANN Model**


*   As stated above, r2 is known as the coefficient of determination and it measures how well the model predicts the outcome or dependent variable (the variable *outputVar*). r2 is a float that ranges from 0 to 1 representing a percent between 0 and 100%. 


"""

## Analyze Accuracy 

# Computing the absolute percent error
APE=100*(abs(TestingData['Energy Output']-TestingData['Predicted Energy Output'])/TestingData['Energy Output'])
TestingData['APE']=APE
 
print('The Accuracy of ANN model is:', 100-np.mean(APE))
TestingData.head()

## Analyze Accuracy - r2 value 

from sklearn.metrics import r2_score as r2_score_sk

r2_ann = r2_score_sk(y_test_orig, Predictions)
r2_ann_percent = round(r2_ann*100, 2)

print("the r2 score is:",r2_ann)
print(r2_ann_percent,"% of the data can be explained by the ANN model")

"""**Figure of Expected and Model Results for ANN**"""

## plot of expected vs model results 
import matplotlib.pyplot as plt

x_axis = list(range(1, len(Predictions)+1))

plt.plot(x_axis, Predictions, 'o g')
plt.plot(x_axis, y_test_orig, 'o b')

title = '(ANN) expected vs model results for ' + outputVar
plt.title(title)
plt.xlabel('datapoint')
plt.ylabel(outputVar)
plt.legend(['Model Predictions', 'Original Data'])

"""**Linear Regression Plot of Predicted and Actual Output Parameter Data - ANN**

"""

## Actual vs Predicted Regression plot 
import seaborn as sns

x = y_test_orig
y = Predictions

# Plot the regression plot
sns.regplot(x, y, label='Predicted')

# Plot the line plot
#sns.lineplot(x, y, label='Predicted')
#sns.lineplot(x[1],y[1], label='Predicted')

# Add a title
plt.title('Actual vs. Predicted (ann)')

# Label the x-axis
plt.xlabel('actual')

# Label the y-axis
plt.ylabel('predicted')

# Add a legend
plt.legend()

# Show the plot
plt.show()

"""**Figure of Expected and Model Results for ANN - Every Nth Datapoint Only**"""

## plot of expected vs model results  - every nth value plotted
# sometimes plotting every datapoint makes it difficult to read the figure, use this section to generate an alternative figure with every nth datapoint

# user input for nth value
nth_stringInput = input('to plot every nth value, please provide an integer value for n (recommended: n = 100)')
nth = int(nth_stringInput)

# plot
x = x_axis[::nth]
y1 = Predictions[::nth]
y2 = y_test_orig[::nth]

plt.plot(x, y1, 'o g')
plt.plot(x, y2, 'o b')

title = '(ANN) expected vs model results for ' + outputVar + " : every " + nth_stringInput + ' datapoints'
plt.title(title)
plt.xlabel('datapoint')
plt.ylabel(outputVar)
plt.legend(['Model Predictions', 'Original Data'])